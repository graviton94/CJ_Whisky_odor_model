import os
import json
import torch
import torch.nn as nn
import numpy as np
import pandas as pd
from pathlib import Path
from torch.utils.data import DataLoader, TensorDataset, WeightedRandomSampler
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.cluster import KMeans

from data_processor import build_input_vector
from config import (
    LEARN_JSONL,
    ODOR_FINETUNE_PTH,
    TASTE_FINETUNE_PTH,
    MODEL_META_ODOR,
    MODEL_META_TASTE,
    TF_BATCH_SIZE,
    TF_EPOCHS,
    TF_LR
)

# Ensure model directory exists
for path in [ODOR_FINETUNE_PTH, TASTE_FINETUNE_PTH, MODEL_META_ODOR, MODEL_META_TASTE]:
    Path(path).parent.mkdir(parents=True, exist_ok=True)

class FinetuneMLP(nn.Module):
    def __init__(self, input_dim, output_dim):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, output_dim),
            nn.Hardtanh(min_val=0.0, max_val=10.0)
        )
    def forward(self, x):
        return self.net(x)


def detect_style_conflicts(X, y, desc_list):
    """
    í•™ìŠµ ë°ì´í„°ì—ì„œ ìŠ¤íƒ€ì¼ ì¶©ëŒ ê°ì§€
    Returns: (has_conflict, clusters, cluster_info)
    """
    if len(X) < 6:  # ìµœì†Œ ë°ì´í„° ìˆ˜
        return False, None, "Insufficient data for conflict detection"
    
    try:
        # íŠ¹ì„± ë²¡í„°ë¡œ í´ëŸ¬ìŠ¤í„°ë§
        kmeans = KMeans(n_clusters=min(3, len(X)//2), random_state=42)
        clusters = kmeans.fit_predict(X)
        
        # í´ëŸ¬ìŠ¤í„°ë³„ descriptor í‰ê·  ê³„ì‚°
        cluster_profiles = {}
        for cluster_id in np.unique(clusters):
            cluster_mask = clusters == cluster_id
            cluster_y = y[cluster_mask]
            avg_scores = np.mean(cluster_y, axis=0)
            cluster_profiles[cluster_id] = {
                'scores': avg_scores,
                'descriptors': {desc_list[i]: avg_scores[i] for i in range(len(desc_list))},
                'count': np.sum(cluster_mask)
            }
        
        # í´ëŸ¬ìŠ¤í„° ê°„ ì°¨ì´ ë¶„ì„
        max_diff = 0
        conflicting_descriptors = []
        
        for i, cluster1 in cluster_profiles.items():
            for j, cluster2 in cluster_profiles.items():
                if i < j:  # ì¤‘ë³µ ë°©ì§€
                    diff = np.abs(cluster1['scores'] - cluster2['scores'])
                    max_diff = max(max_diff, np.max(diff))
                    
                    # í° ì°¨ì´ë¥¼ ë³´ì´ëŠ” descriptor ì°¾ê¸°
                    for idx, d in enumerate(diff):
                        if d > 3.0:  # 3ì  ì´ìƒ ì°¨ì´
                            conflicting_descriptors.append({
                                'descriptor': desc_list[idx],
                                'cluster1_score': cluster1['scores'][idx],
                                'cluster2_score': cluster2['scores'][idx],
                                'difference': d
                            })
        
        has_conflict = max_diff > 3.0 and len(conflicting_descriptors) > 0
        
        conflict_info = {
            'max_difference': max_diff,
            'conflicting_descriptors': conflicting_descriptors,
            'cluster_profiles': cluster_profiles,
            'clusters': clusters
        }
        
        return has_conflict, clusters, conflict_info
        
    except Exception as e:
        return False, None, f"Conflict detection failed: {str(e)}"


def calculate_sample_weights(X, y, clusters=None):
    """
    ìƒ˜í”Œë³„ ê°€ì¤‘ì¹˜ ê³„ì‚° - ìµœê·¼ ë°ì´í„°ì™€ ìœ ì‚¬í•œ ìŠ¤íƒ€ì¼ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜
    ë‹¤ì¤‘ ìŠ¤íƒ€ì¼ ì ì‘: A â†’ B â†’ C ìŠ¤íƒ€ì¼ ë³€í™”ì— ëŒ€ì‘
    """
    weights = np.ones(len(X))
    
    if clusters is not None:
        # ìµœê·¼ 20ê°œ ìƒ˜í”Œ ë¶„ì„ (ë” ê¸´ íŒ¨í„´ ê°ì§€)
        recent_samples = min(20, len(X))
        recent_clusters = clusters[-recent_samples:]
        recent_cluster_counts = np.bincount(recent_clusters)
        
        # ì‹œê°„ ê°€ì¤‘ì¹˜: ë” ìµœê·¼ì¼ìˆ˜ë¡ ë†’ì€ ê°€ì¤‘ì¹˜
        time_decay = np.exp(-np.arange(len(X))[::-1] / (len(X) * 0.3))  # ì§€ìˆ˜ ê°ì†Œ
        
        # í´ëŸ¬ìŠ¤í„° ì§„í™” íŒ¨í„´ ê°ì§€
        cluster_evolution = analyze_cluster_evolution(clusters)
        
        for i, cluster in enumerate(clusters):
            if cluster < len(recent_cluster_counts):
                # 1. ìµœê·¼ ë“±ì¥ ë¹ˆë„
                recent_weight = recent_cluster_counts[cluster] / recent_samples
                
                # 2. ì‹œê°„ ê°€ì¤‘ì¹˜
                temporal_weight = time_decay[i]
                
                # 3. í´ëŸ¬ìŠ¤í„° ì§„í™” ê°€ì¤‘ì¹˜
                evolution_weight = cluster_evolution.get(cluster, 1.0)
                
                # ìµœì¢… ê°€ì¤‘ì¹˜ (0.2~2.0 ë²”ìœ„)
                final_weight = 0.2 + (recent_weight * temporal_weight * evolution_weight) * 1.8
                weights[i] = final_weight
                
        print(f"   ğŸ“Š ê°€ì¤‘ì¹˜ ë¶„í¬: í‰ê· ={np.mean(weights):.2f}, ë²”ìœ„={np.min(weights):.2f}~{np.max(weights):.2f}")
    
    return weights


def analyze_cluster_evolution(clusters):
    """
    í´ëŸ¬ìŠ¤í„° ì§„í™” íŒ¨í„´ ë¶„ì„: ìƒˆë¡œìš´ ìŠ¤íƒ€ì¼ì˜ ë“±ì¥ê³¼ ë³€í™” ê°ì§€
    """
    if len(clusters) < 10:
        return {cluster: 1.0 for cluster in np.unique(clusters)}
    
    # ì‹œê°„ êµ¬ê°„ë³„ í´ëŸ¬ìŠ¤í„° ë¶„í¬ ë¶„ì„
    n_segments = min(5, len(clusters) // 4)
    segment_size = len(clusters) // n_segments
    
    segment_distributions = []
    for i in range(n_segments):
        start_idx = i * segment_size
        end_idx = start_idx + segment_size if i < n_segments - 1 else len(clusters)
        segment = clusters[start_idx:end_idx]
        
        # ê° ì„¸ê·¸ë¨¼íŠ¸ì˜ í´ëŸ¬ìŠ¤í„° ë¶„í¬
        unique, counts = np.unique(segment, return_counts=True)
        distribution = {cluster: count / len(segment) for cluster, count in zip(unique, counts)}
        segment_distributions.append(distribution)
    
    # ì§„í™” ê°€ì¤‘ì¹˜ ê³„ì‚°
    evolution_weights = {}
    all_clusters = np.unique(clusters)
    
    for cluster in all_clusters:
        appearances = []
        for dist in segment_distributions:
            appearances.append(dist.get(cluster, 0.0))
        
        # íŒ¨í„´ ë¶„ì„
        if len(appearances) >= 3:
            recent_trend = np.mean(appearances[-2:])  # ìµœê·¼ 2ê°œ êµ¬ê°„ í‰ê· 
            early_trend = np.mean(appearances[:2])    # ì´ˆê¸° 2ê°œ êµ¬ê°„ í‰ê· 
            
            if recent_trend > early_trend:
                # ìƒìŠ¹ íŠ¸ë Œë“œ (ìƒˆë¡œìš´ ìŠ¤íƒ€ì¼) - ë†’ì€ ê°€ì¤‘ì¹˜
                evolution_weights[cluster] = 1.0 + (recent_trend - early_trend) * 2.0
            elif recent_trend < early_trend * 0.5:
                # ê¸‰ê²©í•œ í•˜ë½ (êµ¬ì‹ ìŠ¤íƒ€ì¼) - ë‚®ì€ ê°€ì¤‘ì¹˜
                evolution_weights[cluster] = 0.5
            else:
                # ì•ˆì •ì  íŒ¨í„´
                evolution_weights[cluster] = 1.0
        else:
            evolution_weights[cluster] = 1.0
    
    return evolution_weights


def multi_style_conflict_analysis(X, y, desc_list):
    """
    ë‹¤ì¤‘ ìŠ¤íƒ€ì¼ ì¶©ëŒ ë¶„ì„ ë° ì „ëµ ê²°ì •
    """
    has_conflict, clusters, conflict_info = detect_style_conflicts(X, y, desc_list)
    
    if not has_conflict:
        return {
            'strategy': 'unified',
            'description': 'ìŠ¤íƒ€ì¼ ì¼ê´€ì„± í™•ì¸ - í†µí•© í•™ìŠµ',
            'clusters': None,
            'weights': np.ones(len(X))
        }
    
    n_clusters = len(np.unique(clusters))
    cluster_evolution = analyze_cluster_evolution(clusters)
    
    # ì „ëµ ê²°ì •
    if n_clusters == 2:
        strategy = 'binary_adaptive'
        description = f'A/B ìŠ¤íƒ€ì¼ ì¶©ëŒ - ìµœê·¼ ìŠ¤íƒ€ì¼ ìš°ì„ '
    elif n_clusters == 3:
        strategy = 'triple_evolution'
        description = f'A/B/C ì‚¼ì¤‘ ìŠ¤íƒ€ì¼ - ì§„í™” íŒ¨í„´ ê¸°ë°˜'
    else:
        strategy = 'multi_cluster'
        description = f'{n_clusters}ê°œ ìŠ¤íƒ€ì¼ í˜¼ì¬ - ë³µí•© ì ì‘'
    
    weights = calculate_sample_weights(X, y, clusters)
    
    return {
        'strategy': strategy,
        'description': description,
        'clusters': clusters,
        'cluster_evolution': cluster_evolution,
        'weights': weights,
        'conflict_info': conflict_info
    }


def train_model(mode: str):
    assert mode in ['odor', 'taste'], "mode must be 'odor' or 'taste'"

    if not os.path.exists(LEARN_JSONL):
        print(f"ğŸš« í•™ìŠµ ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {LEARN_JSONL}")
        return {
            'success': False,
            'reason': f'Learning data file not found: {LEARN_JSONL}'
        }

    # Load JSONL data
    with open(LEARN_JSONL, 'r', encoding='utf-8') as f:
        rows = [json.loads(line) for line in f]

    rows_mode = [r for r in rows if r.get('mode') == mode]
    if not rows_mode:
        print(f"ğŸš« '{mode}' ëª¨ë“œì˜ í•™ìŠµ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return {
            'success': False,
            'reason': f'No training data found for mode: {mode}'
        }

    X_list, y_list = [], []
    desc_list_final = None
    bad_idx = []
    max_dim = 0
    
    # 1ì°¨: ëª¨ë“  rowë¥¼ ì²˜ë¦¬í•˜ì—¬ ìµœëŒ€ ì°¨ì› ì°¾ê¸°
    temp_vectors = []
    dimension_info = {}
    
    for i, row in enumerate(rows_mode):
        try:
            smiles_list = row['smiles_list']
            peak_area = row['peak_area']
            labels = row['labels']
            desc_list = row['desc_list']
            if desc_list_final is None:
                desc_list_final = desc_list
            elif desc_list != desc_list_final:
                raise ValueError(f"Descriptor ìˆœì„œ ë¶ˆì¼ì¹˜! (row {i})")

            # ì‹¤ì œ feature vector ìƒì„± (í•­ìƒ í˜„ì¬ ì•Œê³ ë¦¬ì¦˜ ì‚¬ìš©)
            vec = build_input_vector(smiles_list, peak_area, mode=mode)
            
            # ì°¨ì› ì •ë³´ ìˆ˜ì§‘
            if vec.shape[0] not in dimension_info:
                dimension_info[vec.shape[0]] = 0
            dimension_info[vec.shape[0]] += 1
            
            temp_vectors.append((i, vec, labels))
            max_dim = max(max_dim, vec.shape[0])
            
        except Exception as e:
            print(f"âš ï¸ row {i} ì˜¤ë¥˜: {e}")
            bad_idx.append(i)
    
    print(f"ğŸ“Š ì°¨ì› ë¶„í¬: {dimension_info}")
    print(f"ğŸ“ ìµœëŒ€ ì°¨ì›: {max_dim}")
    
    # JSONLì˜ ì €ì¥ëœ featuresì™€ ì‹¤ì œ ê³„ì‚°ëœ features ë¹„êµ
    mismatch_count = 0
    for i, row in enumerate(rows_mode):
        if 'features' in row and i < len(temp_vectors):
            stored_features = row['features']
            _, actual_vec, _ = temp_vectors[i]
            if len(stored_features) != actual_vec.shape[0]:
                mismatch_count += 1
                print(f"âš ï¸ ì°¨ì› ë¶ˆì¼ì¹˜ ë°œê²¬ row {i}: ì €ì¥ëœ={len(stored_features)}, ì‹¤ì œ={actual_vec.shape[0]}")
    
    if mismatch_count > 0:
        print(f"ğŸ”§ ì´ {mismatch_count}ê°œ rowì—ì„œ ì°¨ì› ë¶ˆì¼ì¹˜ ê°ì§€ - ì‹¤ì œ ê³„ì‚°ê°’ ì‚¬ìš©")
    
    # 2ì°¨: ì°¨ì› í†µì¼í•˜ì—¬ ìµœì¢… ë°ì´í„° êµ¬ì„±
    for i, vec, labels in temp_vectors:
        # ì°¨ì› í†µì¼ (ë¶€ì¡±í•˜ë©´ 0ìœ¼ë¡œ íŒ¨ë”©, ì´ˆê³¼í•˜ë©´ ìë¥´ê¸°)
        if vec.shape[0] < max_dim:
            padding = np.zeros(max_dim - vec.shape[0])
            vec = np.concatenate([vec, padding])
        elif vec.shape[0] > max_dim:
            vec = vec[:max_dim]
        
        X_list.append(vec)
        y_list.append(labels)

    if not X_list:
        print("ğŸš« ìœ íš¨í•œ í•™ìŠµ rowê°€ ì—†ìŠµë‹ˆë‹¤.")
        return {
            'success': False,
            'reason': 'No valid training rows found'
        }

    print(f"ğŸ“Š ì°¨ì› í†µì¼ ì™„ë£Œ: {len(X_list)}ê°œ ìƒ˜í”Œ, ì°¨ì›={max_dim}")
    X = np.stack(X_list)
    y = np.stack(y_list)
    input_dim = X.shape[1]
    output_dim = y.shape[1]

    # ğŸ§ª ìŠ¤íƒ€ì¼ ì¶©ëŒ ê°ì§€
    has_conflict, clusters, conflict_info = detect_style_conflicts(X, y, desc_list_final)
    
    if has_conflict:
        print("âš ï¸ ìŠ¤íƒ€ì¼ ì¶©ëŒ ê°ì§€!")
        print(f"   ìµœëŒ€ ì°¨ì´: {conflict_info['max_difference']:.2f}")
        print(f"   ì¶©ëŒ Descriptor: {[d['descriptor'] for d in conflict_info['conflicting_descriptors']]}")
        
        # ì ì‘ì  ê°€ì¤‘ì¹˜ ê³„ì‚°
        sample_weights = calculate_sample_weights(X, y, clusters)
        print(f"   ê°€ì¤‘ì¹˜ ì ìš©: ìµœê·¼ ìŠ¤íƒ€ì¼ ìš°ì„  (ë²”ìœ„: {np.min(sample_weights):.2f}~{np.max(sample_weights):.2f})")
    else:
        print("âœ… ìŠ¤íƒ€ì¼ ì¼ê´€ì„± í™•ì¸ë¨")
        sample_weights = np.ones(len(X))

    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    X_train, X_val, y_train, y_val = train_test_split(
        X_scaled, y, test_size=0.2, random_state=42
    )
    
    # ê°€ì¤‘ì¹˜ë„ train/valë¡œ ë¶„í• 
    if len(sample_weights) == len(X):
        train_indices = np.arange(len(X_train))
        val_indices = np.arange(len(X_train), len(X))
        weights_train = sample_weights[:len(X_train)]
        weights_val = sample_weights[len(X_train):]
    else:
        weights_train = np.ones(len(X_train))
        weights_val = np.ones(len(X_val))

    # ê°€ì¤‘ì¹˜ë¥¼ ì ìš©í•œ DataLoader ìƒì„±
    if has_conflict:
        # WeightedRandomSampler ì‚¬ìš©
        sampler = WeightedRandomSampler(weights_train, len(weights_train), replacement=True)
        train_loader = DataLoader(
            TensorDataset(
                torch.tensor(X_train, dtype=torch.float32),
                torch.tensor(y_train, dtype=torch.float32)
            ), batch_size=TF_BATCH_SIZE, sampler=sampler
        )
    else:
        train_loader = DataLoader(
            TensorDataset(
                torch.tensor(X_train, dtype=torch.float32),
                torch.tensor(y_train, dtype=torch.float32)
            ), batch_size=TF_BATCH_SIZE, shuffle=True
        )
    
    val_loader = DataLoader(
        TensorDataset(
            torch.tensor(X_val, dtype=torch.float32),
            torch.tensor(y_val, dtype=torch.float32)
        ), batch_size=TF_BATCH_SIZE, shuffle=False
    )

    model = FinetuneMLP(input_dim, output_dim)
    optimizer = torch.optim.Adam(model.parameters(), lr=TF_LR)
    criterion = nn.MSELoss()

    for epoch in range(1, TF_EPOCHS + 1):
        model.train()
        for xb, yb in train_loader:
            pred = model(xb)
            loss = criterion(pred, yb)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

        if epoch % 20 == 0:
            model.eval()
            val_loss = np.mean([
                criterion(model(xb), yb).item() for xb, yb in val_loader
            ])
            print(f"[Epoch {epoch}] Val Loss = {val_loss:.4f}")

    # Save model and metadata
    save_path = ODOR_FINETUNE_PTH if mode == 'odor' else TASTE_FINETUNE_PTH
    meta_path = MODEL_META_ODOR if mode == 'odor' else MODEL_META_TASTE
    torch.save(model.state_dict(), save_path)
    print(f"âœ… Finetuned model saved to {save_path}")

    with open(meta_path, 'w', encoding='utf-8') as f:
        json.dump({
            'input_dim': input_dim,
            'output_dim': output_dim,
            'y_cols': desc_list_final,
            'feature_dim': input_dim
        }, f, ensure_ascii=False, indent=2)
    print(f"âœ… Model meta saved to {meta_path}")

    if bad_idx:
        print(f"âš ï¸ í•™ìŠµ ì œì™¸ëœ row index: {bad_idx}")
    
    # í›ˆë ¨ ì„±ê³µ ê²°ê³¼ ë°˜í™˜
    return {
        'success': True,
        'final_loss': val_loss,
        'input_dim': input_dim,
        'output_dim': output_dim,
        'training_samples': len(X_list),
        'excluded_samples': len(bad_idx),
        'model_path': str(save_path),
        'meta_path': str(meta_path)
    }


if __name__ == '__main__':
    import sys
    # ë°›ì„ ì¸ìê°€ ìˆìœ¼ë©´ ê·¸ ëª¨ë“œë§Œ, ì—†ìœ¼ë©´ odorì™€ taste ëª¨ë‘ ì‹¤í–‰
    modes = sys.argv[1:] if len(sys.argv) > 1 else ['odor', 'taste']
    for mode in modes:
        print(f"\nâ–¶ Running train_model for mode: {mode}\n")
        train_model(mode)